# üñºÔ∏è Proyecto de Super-Resoluci√≥n de Im√°genes (Problema Inverso)

En este proyecto, presentamos una soluci√≥n de resoluci√≥n de super imagen (SR) formulada como un problema inverso mediante la aplicaci√≥n de descenso de gradiente con m√©todos de regularizaci√≥n para obtener resultados de mejor calidad al reconstruir una imagen de alta resoluci√≥n (HR) a partir de una imagen de baja resoluci√≥n (LR) con ruido y degradaci√≥n.

El objetivo es minimizar la funci√≥n de costo:

$$J(\mathbf{x}) = \frac{1}{2} \| \mathbf{A}\mathbf{x} - \mathbf{b} \|_2^2 + \lambda \cdot R(\mathbf{x})$$

Donde:
* $\mathbf{x}$ es la imagen de alta resoluci√≥n a reconstruir.
* $\mathbf{b}$ es la imagen de baja resoluci√≥n de entrada.
* $\mathbf{A}$ es el Operador de Degradaci√≥n (desenfoque + submuestreo).
* $R(\mathbf{x})$ es el t√©rmino de Regularizaci√≥n (suavizado/preservaci√≥n de bordes).
* $\lambda$ es el par√°metro de regularizaci√≥n.

---

## üèóÔ∏è Estructura del Proyecto

El c√≥digo est√° organizado en m√≥dulos Python que encapsulan las principales componentes del problema inverso.

| Archivo | Descripci√≥n Principal | Componentes Clave |
| :--- | :--- | :--- |
| `operators.py` | Implementa el **Operador de Degradaci√≥n** $\mathbf{A}$ y su adjunto $\mathbf{A}^\top$ (para el c√°lculo del gradiente). | `DegradationOperator`, `compute_gradient_x/y`, `compute_divergence`. |
| `regularizers.py` | Define las funciones de costo y sus gradientes para las t√©cnicas de regularizaci√≥n. | **`L2GradientRegularizer`** (Tikhonov), **`HuberGradientRegularizer`** (Preservaci√≥n de bordes). |
| `gradient_descent.py` | Contiene el **Algoritmo de Optimizaci√≥n** para minimizar la funci√≥n de costo $J(\mathbf{x})$. | `SuperResolutionSolver`, M√©todos `compute_cost` y `solve`. |
| `utils.py` | Clases utilitarias para manejo de im√°genes y visualizaci√≥n de resultados/convergencia. | `ImageProcessor`, `PlotUtils`, `ValidationUtils`. |
| `index.html` | Interfaz web (Frontend) para configurar y ejecutar la super-resoluci√≥n (selecci√≥n de imagen, par√°metros). |
| `result.html` | Plantilla para mostrar los resultados de la reconstrucci√≥n, par√°metros y gr√°ficas de convergencia. |

---

## ‚öôÔ∏è Par√°metros de Configuraci√≥n

La reconstrucci√≥n es sensible a los par√°metros del modelo, que pueden configurarse desde la interfaz web (`index.html`):

| Par√°metro | M√≥dulo Relacionado | Descripci√≥n | Valores T√≠picos |
| :--- | :--- | :--- | :--- |
| **Factor Escala** | `DegradationOperator` | Factor por el cual se aumentar√° la resoluci√≥n (ej. 2x, 4x). | 2, 4 |
| **Sigma ($\sigma$)** | `DegradationOperator` | Desviaci√≥n est√°ndar del filtro Gaussiano (magnitud del desenfoque). | 0.5 - 2.0 |
| **Regularizador** | `SuperResolutionSolver` | Tipo de funci√≥n de regularizaci√≥n utilizada. | `L2` (Suavizado), `Huber` (Preserva bordes) |
| **Lambda ($\lambda$)** | `SuperResolutionSolver` | Peso del t√©rmino de regularizaci√≥n. Controla el balance entre fidelidad al dato y suavizado. | 0.001 - 0.1 |
| **Delta ($\delta$)** | `HuberRegularizer` | Umbral para el Regularizador de Huber. Borde m√°s all√° del cual la penalizaci√≥n se vuelve lineal. | 0.01 - 0.5 |
| **Paso ($\tau$)** | `SuperResolutionSolver` | Tasa de aprendizaje (`learning rate`) para el Descenso de Gradiente. | 0.0001 - 0.005 |
| **M√°x. Iteraciones** | `SuperResolutionSolver` | L√≠mite de pasos del algoritmo de Descenso de Gradiente. | 100 - 500 |

---

## üöÄ Uso (Ejemplo Web)

Este proyecto parece estar dise√±ado para ejecutarse dentro de un entorno web (probablemente usando un framework como Flask o Django, a juzgar por el uso de `index.html` y `result.html` con *templating*).

1.  **Iniciar la Aplicaci√≥n Web** (Asumiendo un *backend* existente no incluido):
    ```bash
    # Por ejemplo, si usas Flask:
    python app.py
    ```
2.  **Acceder a la Interfaz:** Navegue a la URL local proporcionada (t√≠picamente `http://127.0.0.1:5000/`).
3.  **Configurar y Ejecutar:**
    * Suba una imagen de baja resoluci√≥n (LR).
    * Ajuste los par√°metros del modelo (Regularizador, $\lambda$, $\tau$, etc.).
    * Haga clic en **"Ejecutar Super-Resoluci√≥n"**.
4.  **Revisar Resultados:** La p√°gina `result.html` mostrar√°:
    * La imagen LR de entrada.
    * La imagen HR reconstruida.
    * La gr√°fica del historial de convergencia (Costo Total, Costo de Datos, Costo de Reg. vs. Iteraci√≥n).

---

## üî¨ Observaciones T√©cnicas

* **Implementaci√≥n de Gradiente:** Las funciones de gradiente en `regularizers.py` y el adjunto del operador de degradaci√≥n en `operators.py` son cruciales para la convergencia del Descenso de Gradiente.
* **Regularizador de Huber:** La implementaci√≥n del regularizador de Huber en `regularizers.py` es fundamental para lograr una buena **preservaci√≥n de bordes** en la imagen reconstruida, ya que penaliza menos los grandes saltos de intensidad (bordes) que el regularizador L2.
* **Manejo de Im√°genes:** `utils.py` se encarga de normalizar las im√°genes a un rango flotante `[0.0, 1.0]`, un paso esencial para el procesamiento num√©rico estable.
